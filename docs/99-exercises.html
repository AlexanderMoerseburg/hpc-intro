<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>99-exercises.utf8</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/accessible-code-block-0.0.1/empty-anchor.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="assets/styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-sm-12 col-md-4 col-lg-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-sm-12 col-md-8 col-lg-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><img id="logo" style="height: 25px;" src="assets/img/logo.svg" /></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="99-setup.html">Setup</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Materials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="01-intro.html">Introduction to HPC</a>
    </li>
    <li>
      <a href="02-working_on_hpc.html">Working on a HPC cluster</a>
    </li>
    <li>
      <a href="03-slurm.html">Using the SLURM Job Scheduler</a>
    </li>
    <li>
      <a href="04-software.html">Managing Software</a>
    </li>
    <li>
      <a href="05-job_arrays.html">Parallelising Jobs</a>
    </li>
    <li>
      <a href="99-cambridge_hpc.html">Cambridge HPC Resources</a>
    </li>
  </ul>
</li>
<li>
  <a href="99-exercises.html">Exercises</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Extras
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="99-unix_cheatsheet.html">Unix Cheatsheet</a>
    </li>
    <li>
      <a href="99-slurm_cheatsheet.html">SLURM Quick Reference</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/cambiotraining">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<hr />
<div class="exercise">
<p>A PhD student wants to process some microscopy data using a python script developed by a postodoc colleague. They have instructions for how to install the necessary python packages, and also the actual python script to process the images.</p>
<p><strong>Q1.</strong> Which of the following describes the best practice for the student to organise their files/software?</p>
<p>Option A:</p>
<pre><code>/scratch/user/project_name/software/ # python packages
/scratch/user/project_name/data/     # image files
/scratch/user/project_name/scripts/  # analysis script</code></pre>
<p>Option B:</p>
<pre><code>/home/user/software/                # python packages
/scratch/user/project_name/data/    # image files 
/scratch/user/project_name/scripts/ # analysis script</code></pre>
<p>Option C:</p>
<pre><code>/home/user/project_name/software/ # python packages
/home/user/project_name/data/        # image files
/home/user/project_name/scripts/     # analysis script</code></pre>
<p><strong>Q2.</strong> It turns out that the microscopy data were very large and compressed as a zip file. The postdoc told the student they can run <code>unzip image_files.zip</code> to decompress the file. Should they run this command from the login node or submit it as a job to one of the compute nodes?</p>
<p><strong>Q3.</strong> The analysis script used by the student generates new versions of the images. In total, after processing the data, the student ends up with ~1TB of data (raw + processed images). Their group still has 5TB of free space on the HPC, so the student decides to keep the data there until they finish the project. Do you agree with this choice, and why? What factors would you take into consideration in deciding what data to keep and where?</p>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>Option C is definitely discouraged: as <code>/home</code> is typically not high-performance and has limited storage, it should not be used for storing/processing data. Option A and B only differ in terms of where the software packages are installed. Typically software can be installed in the user’s <code>/home</code>, avoiding the need to reinstall it multiple times, in case the same software is used in different projects. Therefore, option B is the best practice in this example.</p>
<p><strong>A2.</strong></p>
<p>Since compressing/uncompressing files is a fairly routine task and unlikely to require too many resources, it would be OK to run it on the login node. If in doubt, the student could have gained “interactive” access to one of the compute nodes (we will cover this in another section).</p>
<p><strong>A3.</strong></p>
<p>Leaving the data on the HPC is probably a bad choice. Since typically “scratch” storage is not backed-up it should not be relied on to store important data. If the student doesn’t have access to enough backed-up space for all the data, they should at least back up the raw data and the scripts used to process it. This way, if there is a problem with “scratch” and some processed files are lost, they can recreate them by re-running the scripts on the raw data.</p>
<p>Other criteria that could be used to decide which data to leave on the HPC, backup or even delete is how long each step of the analysis takes to run, as there may be a significant computational cost associated with re-running heavy data processing steps.</p>
</details>
</div>
<hr />
<div class="exercise">
<p>After registering for a HPC account, you were sent the following information by the computing support:</p>
<blockquote>
<p>An account has been created for you on our HPC.</p>
<ul>
<li>Username: emailed separately</li>
<li>Password: emailed separately</li>
<li>Host: <code>oakleaf.bio.cam.ac.uk</code></li>
<li>Port (for file transfer protocols): 22</li>
</ul>
<p>You were automatically allocated 40GB in <code>/home/USERNAME/</code> and 1TB in <code>/scratch/USERNAME/</code>.</p>
</blockquote>
<p><strong>Q1.</strong> Connect to the training HPC using <code>ssh</code></p>
<p><strong>Q2.</strong> Take some time to explore your home directory to identify what files and folders are in there. Can you identify and navigate to your scratch directory?</p>
<p><strong>Q3.</strong> Create a directory called <code>hpc_workshop</code> in your “scratch” directory.</p>
<p><strong>Q4.</strong> Using the commands <code>free -h</code> (available RAM memory) and <code>nproc --all</code> (number of CPU cores available) compare the capabilities of your own computer to the capabilities of the login node of our HPC. Check how many people are logged in to the HPC login node using the command <code>who</code>.</p>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>To login to the HPC we run the following from the terminal:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">ssh</span> USERNAME@oakleaf.bio.cam.ac.uk</span></code></pre></div>
<p>(replace “USERNAME” by your HPC username)</p>
<p><strong>A2.</strong></p>
<p>We can get a detailed list of the files on our home directory:</p>
<pre class="console"><code>ls -l</code></pre>
<p>This will reveal that there is a shell script (<code>.sh</code> extension) named <code>slurm_submit_template.sh</code> and also a shortcut to our scratch directory. We can see that this is a shortcut because of the way the output is printed as <code>scratch -&gt; /scratch/username/</code>.</p>
<p>Therefore, to navigate to our scratch directory we can either use the shortcut from our home or use the full path:</p>
<pre class="console"><code>cd ~/scratch       # using the shortcut from the home directory
cd /scratch/user/  # using the full path</code></pre>
<p>Remember that <code>~</code> indicates your home directory.</p>
<p><strong>A3.</strong></p>
<p>Once we are in the scratch directory, we can use <code>mkdir</code> to create our workshop sub-directory:</p>
<pre class="console"><code>mkdir hpc_workshop</code></pre>
<p><strong>A4.</strong></p>
<p>The main thing to consider in this question is where you run the commands from. To get the number of CPUs and memory on your computer make sure you open a new terminal and that you see something like <code>[your-local-username@laptop: ~]$</code> (where “user” is the username on your personal computer and “laptop” is the name of your personal laptop).</p>
<p>Conversely, to obtain the same information for the HPC, make sure you are logged in to the HPC when you run the commands. You should see something like <code>[your-hpc-username@login ~]$</code></p>
<p>To see how many people are currently on the login node we can combine the <code>who</code> and <code>wc</code> commands:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># pipe the output of `who` to `wc`</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="co"># the `-l` flag instructs `wc` to count &quot;lines&quot; of its input</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="fu">who</span> <span class="kw">|</span> <span class="fu">wc</span> -l</span></code></pre></div>
</details>
</div>
<hr />
<div class="exercise">
<p>If you haven’t already done so, connect your VS Code to the HPC following the instructions in Figure 2.</p>
<ol style="list-style-type: decimal">
<li>Open the <code>hpc_workshop</code> folder on VS Code (this is the folder you created in the previous exercise).</li>
<li>Create a new file (File &gt; New File) and save it as <code>test.sh</code>. Copy the code shown below into this script and save it.</li>
<li>From the terminal, run this script with <code>bash test.sh</code></li>
</ol>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="bu">echo</span> <span class="st">&quot;This job is running on:&quot;</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="fu">hostname</span></span></code></pre></div>
<details>
<p><summary>Answer</summary> <strong>A1.</strong></p>
<p>To open the folder we follow the instructions in Figure 3 and use the following path: <code>/scratch/user/hpc_workshop</code> (replacing “user” with your username)</p>
<p><strong>A2.</strong></p>
<p>To create a new script in VS Code we can go to “File &gt; New File” or use the <kbd>Ctrl + N</kbd> shortcut.</p>
<p><strong>A3.</strong></p>
<p>We can run the script from the terminal. First make sure you are on the correct folder:</p>
<pre class="console"><code>cd /scratch/user/hpc_workshop</code></pre>
<p>Then run the script:</p>
<pre class="console"><code>bash scripts/test.sh</code></pre>
</details>
</div>
<hr />
<div class="exercise">
<ul>
<li><a href="">Download the data</a> for this course to your computer and place it on your Desktop.</li>
<li>Use <em>Filezilla</em>, <code>scp</code> or <code>rsync</code> (your choice) to move this file to the directory we created earlier: <code>/scratch/user/hpc_workshop/</code>.</li>
<li>The file we just downloaded is a compressed file. From the HPC terminal, use <code>unzip</code> to decompress the file.</li>
<li>Bonus: how many shell scripts (with <code>.sh</code> extension) are there in your project folder?</li>
</ul>
<details>
<p><summary>Answer</summary></p>
<p>Once we download the data to our computer, we can transfer it using either of the suggested programs. We show the solution using command-line tools.</p>
<p>Notice that these commands are <strong>run from your local terminal</strong>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># with scp</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">scp</span> -r ~/Desktop/hpc_workshop_files.zip username@oakleaf.bio.cam.ac.uk:scratch/hpc_workshop/</span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="co"># with rsync</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="fu">rsync</span> -avhu ~/Desktop/hpc_workshop_files.zip username@oakleaf.bio.cam.ac.uk:scratch/hpc_workshop/</span></code></pre></div>
<p>Once we finish transfering the files we can go ahead and decompress the data folder. Note, this is now run <strong>from the HPC terminal</strong>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># make sure to be in the correct directory</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="bu">cd</span> ~/scratch/hpc_workshop/</span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># decompress the files</span></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="fu">unzip</span> hpc_workshop_files.zip</span></code></pre></div>
<p>Finally, we can check how many shell scripts there are using the <code>find</code> program and piping it to the <code>wc</code> (word/line count) program:</p>
<p><code>find -type f -name *.sh | wc -l</code></p>
<p><code>find</code> is a very useful tool to find files, check this <a href="https://devhints.io/find">Find cheatsheet</a> to learn more about it.</p>
</details>
</div>
<hr />
<div class="exercise">
In the “scripts” directory, you will find an R script called <code>pi_estimator.R</code>. This script tries to get an approximate estimate for the number Pi using a stochastic algorithm.
<details>
<p><summary>How does the algorithm work?</summary></p>
<p>If you are interested in the details, here is a short description of what the script does:</p>
<blockquote>
<p>The program generates a large number of random points on a 1×1 square centered on (½,½), and checks how many of these points fall inside the unit circle. On average, π/4 of the randomly-selected points should fall in the circle, so π can be estimated from 4f, where f is the observed fraction of points that fall in the circle. Because each sample is independent, this algorithm is easily implemented in parallel.</p>
</blockquote>
<div class="figure">
<img src="https://carpentries-incubator.github.io/hpc-intro/fig/pi.png" style="width:50.0%" alt="" />
<p class="caption">Estimating Pi by randomly placing points on a quarter circle. (Source: <a href="https://carpentries-incubator.github.io/hpc-intro/16-parallel/index.html">HPC Carpentry</a>)</p>
</div>
</details>
<p>If you were running this script interactively (i.e. directly from the console), you would use the R script interpreter: <code>Rscript scripts/pi_estimator.R</code>. Instead, we use a shell script to submit this to the job scheduler.</p>
<ol style="list-style-type: decimal">
<li>Edit the shell script in <code>slurm/estimate_pi.sh</code> by correcting the code where the word “FIXME” appears. Submit the job to SLURM and check its status in the queue.</li>
<li>How long did the job take to run and how many resources did it use?
<details>
<summary>Hint</summary>Use <code>seff JOBID</code> or <code>scontrol show JOBID</code>.
</details></li>
<li>The number of samples used to estimate Pi can be modified using the <code>--nsamples</code> option of our script, defined in millions. The more samples we use, the more precise our estimate should be.
<ul>
<li>Adjust your SLURM submission script to use 500 million samples (<code>Rscript scripts/pi_estimator.R --nsamples 500</code>), and save the job output in <code>logs/estimate_pi_500M.log</code>.</li>
<li>Monitor the job status with <code>squeue</code> and <code>scontrol show JOBID</code>. <!-- If you find any issues, how would you fix them? --></li>
</ul></li>
</ol>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>In the shell script we needed to correct the user-specific details in the <code>#SBATCH</code> options. Also, we needed to specify the path to the script we wanted to run. This can be defined relative to the working directory that we’ve set with <code>-D</code>. For example:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="co">#SBATCH -p training </span></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="co">#SBATCH -D /scratch/USERNAME/hpc_workshop/  # working directory</span></span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="co">#SBATCH -o logs/estimate_pi.log  # standard output file</span></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="co">#SBATCH -c 1        # number of CPUs. Default: 1</span></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="co">#SBATCH -t 00:10:00 # time for the job HH:MM:SS.</span></span>
<span id="cb14-7"><a href="#cb14-7"></a></span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="co"># run the script</span></span>
<span id="cb14-9"><a href="#cb14-9"></a><span class="ex">Rscript</span> scripts/pi_estimator.R</span></code></pre></div>
<p><strong>A2.</strong></p>
<p>As suggested in the hint, we can use the <code>seff</code> or <code>scontrol</code> commands for this:</p>
<pre class="console"><code>seff JOBID
scontrol show JOBID</code></pre>
<p>Replacing JOBID with the ID of the job we just ran.</p>
<p>If you cannot remember what the job id was, you can run <code>sacct</code> with no other options and it will list the last few jobs that you ran.</p>
<p>Strangely enough, the “Memory Utilized” is reported as 0.00MB. That’s very odd, since for sure our script must have used <em>some</em> memory to do the computation. The reason is that SLURM doesn’t always have time to pick memory usage spikes, and so it reports a zero. This is usually not an issue with longer-running jobs.</p>
<p><strong>A3.</strong></p>
<p>The modified script should look similar to this:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="co">#SBATCH -p training </span></span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="co">#SBATCH -D /scratch/USERNAME/hpc_workshop/  # working directory</span></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="co">#SBATCH -o logs/estimate_pi.log  # standard output file</span></span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="co">#SBATCH -c 1        # number of CPUs. Default: 1</span></span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="co">#SBATCH -t 00:10:00 # time for the job HH:MM:SS.</span></span>
<span id="cb16-7"><a href="#cb16-7"></a></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="co"># run the script</span></span>
<span id="cb16-9"><a href="#cb16-9"></a><span class="ex">Rscript</span> scripts/pi_estimator.R --nsamples 500</span></code></pre></div>
<p>However, when we run this job, examining the output file (<code>cat logs/estimate_pi_500M.log</code>) will reveal:</p>
<pre><code>slurmstepd: error: Detected 1 oom-kill event(s) in step JOBID.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.</code></pre>
<p>Furthermore, if we use <code>seff</code> to get information about the job, it will show <code>State: OUT_OF_MEMORY (exit code 0)</code>.</p>
<p>This suggests that the job required more memory than we requested. To correct this problem, we would need to increase the memory requested to SLURM, adding to our script, for example, <code>#SBATCH --mem=10G</code> to request 10Gb of RAM memory for the job.</p>
<p>Again, <code>seff</code> is rather unhelpful in accurately reporting how much memory the job used. Clearly, it ran out of memory, but because it ran so fast SLURM didn’t register the memory usage peak.</p>
</details>
</div>
<hr />
<div class="exercise">
<p>The R script used in the previous exercise supports parallelisation of some of its internal computations. The number of CPUs used by the script can be modified using the <code>--ncpus</code> option. For example <code>pi_estimator.R --ncpus 2</code> would use two CPUs.</p>
<ol style="list-style-type: decimal">
<li>Modify your submission script to use the <code>$SLURM_CPUS_PER_TASK</code> variable to set the number of CPUs used by <code>pi_estimator.R</code>.</li>
<li>Submit the job a few times, each one using 1, 2 and then 8 CPUs. Make a note of each job’s ID.</li>
<li>Check how much time each job took to run (using <code>scontrol show job JOBID</code>). Did increasing the number of CPUs shorten the time it took to run?</li>
</ol>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>We can modify our submission script in the following manner:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="co">#SBATCH -p training     # partiton name</span></span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="co">#SBATCH -D /scratch/</span><span class="al">FIXME</span><span class="co">/hpc_workshop/  # working directory</span></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="co">#SBATCH -o logs/estimate_pi.log      # output file</span></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="co">#SBATCH --mem=10G</span></span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="co">#SBATCH -c 2                          # number of CPUs</span></span>
<span id="cb18-7"><a href="#cb18-7"></a></span>
<span id="cb18-8"><a href="#cb18-8"></a><span class="co"># launch the Pi estimator script using the number of CPUs that we are requesting from SLURM</span></span>
<span id="cb18-9"><a href="#cb18-9"></a><span class="ex">Rscript</span> exercises/pi_estimator.R --nsamples 500 --ncpus <span class="va">$SLURM_CPUS_PER_TASK</span></span></code></pre></div>
<p>We can run the job multiple times by modifying the <code>#SBATCH -c</code> option.</p>
<p>After running each job we can use <code>scontrol show job JOBID</code> command to obtain information about how long it took to run.</p>
<p><code>seff JOBID</code></p>
<!--
After running each job we can use the `seff` command to obtain information about how long it took to run:

`seff JOBID`

Alternatively, since we want to compare several jobs, we could also use `sacct`:

`sacct -o JobID,elapsed -j JOBID1,JOBID2,JOBID3`
-->
<p>In this case, it does seem that increasing the number of CPUs shortens the time the job takes to run. However, the increase is not linear at all. Going from 1 to 2 CPUs speeds things up a bit, but beyond that we don’t get much better performance. This is possibly because there are other computational costs to do with this kind of parallelisation (e.g. keeping track of what each parallel thread is doing).</p>
</details>
</div>
<hr />
<div class="exercise">
<p>In the <code>hpc_workshop/data</code> folder, you will find some files resulting from whole-genome sequencing individuals from the model organism <em>Drosophila melanogaster</em> (fruit fly). Our objective will be to align our sequences to the reference genome, using a software called <em>bowtie2</em>.</p>
<p><img src="images/mapping.png" style="width:50.0%" /></p>
<p>But first, we need to prepare our genome for this alignment procedure (this is referred to as indexing the genome). We have a file with the <em>Drosophila</em> genome in <code>data/genome/drosophila_genome.fa</code>.</p>
<ol style="list-style-type: decimal">
<li>Create a new conda environment named “bioinformatics”.
<details>
<summary>Hint</summary>Remember the syntax to create a new environment is: <code>conda create --name ENV</code>
</details></li>
<li>Install the <code>bowtie2</code> program in your new environment.
<details>
<summary>Hint</summary>Go to <a href="https://anaconda.org/">anaconda.org</a> and search for “bowtie2” to confirm it is available through <em>Conda</em> and which software <em>channel</em> it is provided from. Remember that you can install packages using <code>conda install --channel CHANNEL-NAME --name ENVIRONMENT-NAME SOFTWARE-NAME</code>.
</details></li>
<li>Check that the software installed correctly by running <code>which bowtie2</code> and <code>bowtie2 --help</code>.
<details>
<summary>Hint</summary>Remember to activate your environment first with <code>source activate bioinformatics</code>.
</details></li>
<li>Open the script in <code>hpc_workshop/slurm/drosophila_genome_indexing.sh</code> and edit the <code>#SBATCH</code> options with the word “FIXME”. Submit the script to SLURM using <code>sbatch</code>, check it’s progress, and whether it ran successfully. Troubleshoot any issues that may arise.</li>
</ol>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>To create a new conda environment we run:</p>
<pre class="console"><code>$ conda create --name bioinformatics</code></pre>
<p><strong>A2.</strong></p>
<p>If we search for this software on the <em>Anaconda</em> website, we will find that it is available via the “<em>bioconda</em>” channel: <a href="https://anaconda.org/bioconda/bowtie2" class="uri">https://anaconda.org/bioconda/bowtie2</a></p>
<p>We can install it on our environment with:</p>
<pre class="console"><code>$ conda install --name bioinformatics --channel bioconda bowtie2</code></pre>
<p><strong>A3.</strong></p>
<p>First we need to activate our environment:</p>
<pre class="console"><code>$ source activate bioinformatics</code></pre>
<p>Then, if we run <code>bowtie2 --help</code>, we should get the software help printed on the console.</p>
<p><strong>A4.</strong></p>
<p>We need to fix the script to specify the correct working directory with our username (only showing the relevant line of the script):</p>
<pre><code>#SBATCH -D /scratch/USERNAME/hpc_workshop</code></pre>
<p>Replacing “USERNAME” with your username.</p>
<p>We can then launch it with sbatch, making sure that we’re in the correct directory on the HPC:</p>
<pre class="console"><code>$ cd /scratch/USERNAME/hpc_workshop

$ sbatch slurm/drosophila_genome_indexing.sh</code></pre>
<p>We can check the job status by using <code>squeue -u USERNAME</code>. And we can obtain more information by using <code>seff JOBID</code> or <code>scontrol show job JOBID</code>.</p>
<p>From this, we should realise that the job has failed. Examining the output log file (<code>cat logs/drosophila_genome_indexing.log</code>), we will notice that we have the following error:</p>
<pre><code></code></pre>
<p>This is because we did not load the conda environment in our script. Remember that even though we may have loaded the environment on the login node, the scripts are run on a different machine (one of the compute nodes), so we need to remember to <strong>always load the conda environment in our SLURM submission scripts</strong>.</p>
<p>We could modify our script by adding the line of code <code>source activate bioinformatics</code> before the rest of the code. Here is the complete script:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="co"># #SBATCH -A training                      # the account for billing</span></span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="co">#SBATCH -J index_genome                  # job name</span></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="co">#SBATCH -D /rds/user/hm533/hpc-work/hpc_workshop</span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="co"># #SBATCH -D /scratch/</span><span class="al">FIXME</span><span class="co">/hpc_workshop/  # working directory</span></span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="co">#SBATCH -o logs/drosophila_genome_indexing.log</span></span>
<span id="cb25-7"><a href="#cb25-7"></a><span class="co">#SBATCH -p cclake                        # queue name</span></span>
<span id="cb25-8"><a href="#cb25-8"></a><span class="co">#SBATCH -c 1                             # CPUs to use</span></span>
<span id="cb25-9"><a href="#cb25-9"></a><span class="co">#SBATCH --mem=1G                         # Memory to use</span></span>
<span id="cb25-10"><a href="#cb25-10"></a><span class="co">#SBATCH -t 00:10:00                      # Time for the job in HH:MM:SS</span></span>
<span id="cb25-11"><a href="#cb25-11"></a></span>
<span id="cb25-12"><a href="#cb25-12"></a><span class="co"># load conda environment</span></span>
<span id="cb25-13"><a href="#cb25-13"></a><span class="bu">source</span> activate bioinformatics</span>
<span id="cb25-14"><a href="#cb25-14"></a></span>
<span id="cb25-15"><a href="#cb25-15"></a><span class="co"># make a directory for the reference</span></span>
<span id="cb25-16"><a href="#cb25-16"></a><span class="fu">mkdir</span> -p results/drosophila/genome</span>
<span id="cb25-17"><a href="#cb25-17"></a></span>
<span id="cb25-18"><a href="#cb25-18"></a><span class="co"># index the reference genome with bowtie2; the syntax is:</span></span>
<span id="cb25-19"><a href="#cb25-19"></a><span class="co"># bowtie2-build input.fa output_prefix</span></span>
<span id="cb25-20"><a href="#cb25-20"></a><span class="ex">bowtie2-build</span> data/drosophila_genome.fa results/drosophila/genome/drosophila</span></code></pre></div>
<p>Re-running it should complete successfully and we should get several output files in the directory <code>results/drosophila/genome</code> with an extension “.bt2”:</p>
<pre class="console"><code>$ ls results/drosophila/genome</code></pre>
<pre><code>drosophila.1.bt2
drosophila.2.bt2
drosophila.3.bt2
drosophila.4.bt2
drosophila.rev.1.bt2
drosophila.rev.2.bt2</code></pre>
</details>
</div>
<hr />
<div class="exercise">
<p>Previously, we used the <code>pi_estimator.R</code> script to obtain an estimate of the number Pi. Since this is done using a stochastic algorithm, we may want to run it several times to get a sense of the error associated with our estimate.</p>
<ol style="list-style-type: decimal">
<li>Use <em>VS Code</em> to open the SLURM submission script in <code>slurm/parallel_estimate_pi.sh</code>. Adjust the <code>#SBATCH</code> options, to run the job 10 times using a job array.</li>
<li>Launch the job with <code>sbatch</code>, monitor its progress and examine the output.
<details>
<summary>Hint</summary> Note that the output of <code>pi_estimator.R</code> is now being <em>appended</em> to a text file with <code>pi_estimator.R &gt;&gt; results/pi_estimates.txt</code>. So the output of all the 100 jobs of the array will be written to this same file, one after the other.
</details></li>
</ol>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>In our script, we need to add <code>#SBATCH -a 1-10</code> as one of our options, so that when we submit this scrit to <code>sbatch</code>, it will run 100 iterations of it in parallel.</p>
<p><strong>A2.</strong></p>
<p>We can launch our adjusted script with <code>sbatch slurm/parallel_estimate_pi.sh</code>. When we check our jobs with <code>squeue -u USERNAME</code>, we will notice several jobs with JOBID in the format “ID_1”, “ID_2”, etc. These indicate the number of the array that is currently running as part of that job submission.</p>
<p>In this case, we only specified a single output log file in <code>#SBATCH -o</code> (we did not use the <code>%a</code> keyword). So all the information about the jobs was sent to a single file in <code>logs/parallel_estimate_pi.log</code>.</p>
<p>However, for our actual estimate of Pi, we redirected (<code>&gt;&gt;</code>) the output to a text file in <code>results/pi_estimate.txt</code>. If we examine this file (e.g. with <code>less results/pi_estimate.txt</code>) we can see it has the results of all the runs of our simulation.</p>
</details>
</div>
<hr />
<div class="exercise">
A PhD student is working on project to understand how different patterns, such as animal stripes and coral colonies, form in nature. They are using a type of model, first proposed by <a href="https://en.wikipedia.org/wiki/Turing_pattern">Alan Turing</a>, which models the interaction between two components that can difuse in space and promote/inhibit each other.
<details>
<p><summary>More</summary></p>
<p>Turing patterns can be generated with a type of mathematical model called a “Reaction-diffusion system”. It models two substances - A and B - that can difuse in space and interact with each other in the following way: substance A self-activates itself and also activates B; B inhibits A.</p>
<div class="figure">
<img src="https://ars.els-cdn.com/content/image/3-s2.0-B9780123821904000061-f06-05-9780123821904.jpg" alt="" />
<p class="caption"><a href="https://doi.org/10.1016/B978-0-12-382190-4.00006-1" class="uri">https://doi.org/10.1016/B978-0-12-382190-4.00006-1</a></p>
</div>
<p>This seemingly simple interaction can generate complex spatial patterns, some of which capture the diversity of patterns observed in nature. Here is a very friendly video illustrating this: <a href="https://youtu.be/alH3yc6tX98" class="uri">https://youtu.be/alH3yc6tX98</a></p>
</details>
<p>They have a python script which runs this model and produces an output image as exemplified above. The two main parameters in the model are called “feed” and “kill”, and their python script accepts these as options, for example:</p>
<pre class="console"><code>$ python scripts/turing_model.py --feed 0.04 --kill 0.06 --outdir results/turing/</code></pre>
<p>This would produce an image saved as <code>results/turing/f0.04_k0.06.png</code>.</p>
<p>The student has been running this script on their laptop, but it takes a while to run and they would like to try several parameter combinations. They have prepared a CSV file in <code>data/turing_model_parameters.csv</code> with parameter values of interest (you can open this file in <em>VS Code</em> to quickly inspect its contents).</p>
<p>Our objective is to automate running these models in parallel on the HPC.</p>
<ol style="list-style-type: decimal">
<li>Use <em>VS Code</em> to open the SLURM submission script in <code>slurm/parallel_turing_pattern.sh</code>. The first few lines of the code are used to fetch parameter values from the CSV file, using the special <code>$SLURM_ARRAY_TASK_ID</code> variable. Fix the <code>#SBATCH -a</code> option to get these values from the CSV file.
<details>
<summary>Hint</summary>The array should have as many numbers as there are lines in our CSV file. However, make sure the array number starts at 2 because the CSV file has a header with column names.
</details></li>
<li>Launch the job with <code>sbatch</code> and monitor its progress (<code>squeue</code>), whether it runs successfully (<code>scontrol show job</code>), and examine the SLURM output log files.</li>
<li>Examine the output files in the <code>results/turing/</code> folder (Note: you can preview image files by opening them in <em>VS Code</em>.)</li>
</ol>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>Our array numbers should be: <code>#SBATCH -a 2-5</code>. We start at 2, because the parameter values start at the second line of the parameter file. We finish at 5, because that’s the number of lines in the CSV file.</p>
<p><strong>A2.</strong></p>
<p>We can submit the script with <code>sbatch slurm/parallel_turing_pattern.sh</code>. While the job is running we can monitor its status with <code>squeue -u USERNAME</code>. We should see several jobs listed with IDs as <code>JOBID_ARRAYID</code> format.</p>
<p>Because we used the <code>%a</code> keyword in our <code>#SBATCH -o</code> option, we will have an output log file for each job of the array. We can list these log files with <code>ls logs/parallel_turing_pattern_*.log</code> (using the "*" wildcard to match any character). If we examine the content of one of these files (e.g. <code>cat logs/parallel_turing_pattern_1.log</code>), we should only see the messages we printed with the <code>echo</code> commands. The actual output of the python script is an image, which is saved into the <code>results/turing</code> folder.</p>
<p><strong>A3.</strong></p>
<p>Once all the array jobs finish, we should have 5 image files in <code>ls results/turing</code>. We can open these images from within <em>VS Code</em>, or alternatively we could move them to our computer with <em>Filezilla</em> (or the command-line <code>scp</code> or <code>rsync</code> commands), as we covered in the <a href="02-working_on_hpc.html#Moving_Files">Moving Files Session</a>.</p>
</details>
</div>
<hr />
<div class="exercise">
<p>(This is a bioinformatics-flavoured version of the previous exercise.)</p>
<p>Continuing from our previous exercise where we <a href="04-software.html#Loading_Conda_Environments">prepared our <em>Drosophila</em> genome for bowtie2</a>, we now want to map each of our samples’ sequence data to the reference genome.</p>
<p><img src="images/mapping.png" style="width:50.0%" /></p>
<p>Looking at our data directory (<code>ls hpc_workshop/data/reads</code>), we can see several sequence files in standard <em>fastq</em> format. These files come in pairs (with suffix "_1" and "_2"), and we have 8 different samples. Ideally we want to process these samples in parallel in an automated way.</p>
<p>We have created a CSV file with three columns. One column contains the sample’s name (which we will use for our output files) and the other two columns contain the path to the first and second pairs of the input files. With the information on this table, we should be able to automate our data processing using a SLURM job array.</p>
<ol style="list-style-type: decimal">
<li>Use <em>VS Code</em> to open the SLURM submission script in <code>slurm/parallel_drosophila_mapping.sh</code>. The first few lines of the code are used to fetch parameter values from the CSV file, using the special <code>$SLURM_ARRAY_TASK_ID</code> variable. Fix the <code>#SBATCH -a</code> option to get these values from the CSV file.
<details>
<summary>Hint</summary>The array should have as many numbers as there are lines in our CSV file. However, make sure the array number starts at 2 because the CSV file has a header with column names.
</details></li>
<li>Launch the job with <code>sbatch</code> and monitor its progress (<code>squeue</code>), whether it runs successfully (<code>scontrol show job</code>), and examine the SLURM output log files.</li>
<li>Examine the output files in the <code>results/drosophila/mapping</code> folder. (Note: the output files are text-based, so you can examine them by using the command line program <code>less</code>, for example.)</li>
</ol>
<details>
<p><summary>Answer</summary></p>
<p><strong>A1.</strong></p>
<p>Our array numbers should be: <code>#SBATCH -a 2-9</code>. We start at 2, because the parameter values start at the second line of the parameter file. We finish at 9, because that’s the number of lines in the CSV file.</p>
<p><strong>A2.</strong></p>
<p>We can submit the script with <code>sbatch slurm/parallel_drosophila_mapping.sh</code>. While the job is running we can monitor its status with <code>squeue -u USERNAME</code>. We should see several jobs listed with IDs as <code>JOBID_ARRAYID</code> format.</p>
<p>Because we used the <code>%a</code> keyword in our <code>#SBATCH -o</code> option, we will have an output log file for each job of the array. We can list these log files with <code>ls logs/parallel_drosophila_mapping_*.log</code> (using the "*" wildcard to match any character). If we examine the content of one of these files (e.g. <code>cat logs/parallel_drosophila_mapping_1.log</code>), we should only see the messages we printed with the <code>echo</code> commands. The actual output of the <code>bowtie2</code> program is a file in [SAM](<a href="https://en.wikipedia.org/wiki/SAM_(file_format)" class="uri">https://en.wikipedia.org/wiki/SAM_(file_format)</a> format, which is saved into the <code>results/drosophila/mapping</code> folder.</p>
<p><strong>A3.</strong></p>
<p>Once all the array jobs finish, we should have 8 SAM files in <code>ls results/drosophila/mapping</code>. We can examine the content of these files, although they are not terribly useful by themselves. In a typical bioinformatics workflow these files would be used for further analysis, for example SNP-calling.</p>
</details>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
